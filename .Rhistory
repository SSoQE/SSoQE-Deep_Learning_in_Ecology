import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score
import matplotlib.pyplot as plt
from typing import List, Dict
#load the data set into the python environment
df = pd.read_csv("C:/Users/bt308479/Documents/GIT/SSoQE-Deep_Learning_in_Ecology/Data/Processed/mushrooms_numerical.csv")
#select the columns that still have to be hotencoded
df_to_encode = df.drop(["class", "bruises"], axis=1)
print(df_to_encode.head())
#hot-encode the categorical columns using a pandas costum function
df_hotencoded = pd.get_dummies(df_to_encode)
print(df_hotencoded.head())
#first split: 60% train, 40% temp
inp_train, inp_temp, out_train, out_temp = train_test_split(inp, out, test_size = 0.4, random_state = 42, stratify = out) #random_state ensures reproducibility, stratify the preservation
#second split: split thr 40% temp into 20% validation and 20% test data
inp_val, inp_test, out_val, out_test = train_test_split(inp_temp, out_temp, test_size = 0.5, random_state = 42, stratify = out_temp)
# Convert to PyTorch tensors
inp_train = torch.tensor(inp_train.values, dtype=torch.float32)
out_train = torch.tensor(out_train.values, dtype=torch.long)
inp_val   = torch.tensor(inp_val.values, dtype=torch.float32)
out_val   = torch.tensor(out_val.values, dtype=torch.long)
inp_test  = torch.tensor(inp_test.values, dtype=torch.float32)
out_test  = torch.tensor(out_test.values, dtype=torch.long)
# Quick check
print("Train:", inp_train.shape, out_train.shape)
print("Val:  ", inp_val.shape, out_val.shape)
print("Test: ", inp_test.shape, out_test.shape)
#save the classes to be predicted in a seperate variable
out = df["class"]
#and add the boolean column "bruises" to the hot encoded categories
inp = df_hotencoded.copy()
inp["bruises"] = df["bruises"]
#first split: 60% train, 40% temp
inp_train, inp_temp, out_train, out_temp = train_test_split(inp, out, test_size = 0.4, random_state = 42, stratify = out) #random_state ensures reproducibility, stratify the preservation
#second split: split thr 40% temp into 20% validation and 20% test data
inp_val, inp_test, out_val, out_test = train_test_split(inp_temp, out_temp, test_size = 0.5, random_state = 42, stratify = out_temp)
# Convert to PyTorch tensors
inp_train = torch.tensor(inp_train.values, dtype=torch.float32)
out_train = torch.tensor(out_train.values, dtype=torch.long)
inp_val   = torch.tensor(inp_val.values, dtype=torch.float32)
out_val   = torch.tensor(out_val.values, dtype=torch.long)
inp_test  = torch.tensor(inp_test.values, dtype=torch.float32)
out_test  = torch.tensor(out_test.values, dtype=torch.long)
# Quick check
print("Train:", inp_train.shape, out_train.shape)
print("Val:  ", inp_val.shape, out_val.shape)
print("Test: ", inp_test.shape, out_test.shape)
input_size = inp_train.shape[1]  # number of one-hot features
hidden_size = 64                 # a random power of two that it's not too big
output_size = 1
model = nn.Sequential(
#definition of input layer, nn.Linear is a fully connected layer, the first parameter defines the number of input nodes, the second the number of output nodes to which the input nodes are connected
nn.Linear(input_size, hidden_size),
#non-linear activation function, LeakyReLU allows a small gradient when the input is negative
nn.LeakyReLU(),
nn.Linear(hidden_size, hidden_size),           # hidden → hidden
nn.LeakyReLU(),
nn.Linear(hidden_size, output_size),            # hidden → output
#activation function for binary classificaiton; returns probability value between 0 and 1, where 0 means it's definetly edible and 1 it's for sure poisnous
nn.Sigmoid()
)
loss_function = nn.BCELoss()
learning_rate = 0.001
optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)
epochs = 5
bsize = 8000                 #batch size
train_dataset = TensorDataset(inp_train, out_train)
val_dataset = TensorDataset(inp_val, out_val)
train_loader = DataLoader(train_dataset, batch_size = bsize, shuffle = True)
val_loader = DataLoader(val_dataset, batch_size = bsize)
#| message: false
train_losses = []
val_losses = []
#procedure conducted for each epoch
for epoch in range(epochs):
model.train()
running_loss = 0.0
#procedure conducted for each batch that's created by the train_loader
for X_batch, y_batch in train_loader:
#model prediction on the current training batch
outputs = model(X_batch).squeeze()
#loss computed based on the model prediction and the true labels
loss = loss_function(outputs, y_batch.float())
#backward propagation that updates the weights and biases in the DL model
optimizer.zero_grad()
loss.backward()
optimizer.step()
running_loss += loss.item()
avg_train_loss = running_loss / len(train_loader)
train_losses.append(avg_train_loss)
#Validation
model.eval()
val_loss = 0.0
with torch.no_grad():
#computing validation loss for each batch
for X_batch, y_batch in val_loader:
outputs = model(X_batch).squeeze()
loss = loss_function(outputs, y_batch.float())
val_loss += loss.item()
avg_val_loss = val_loss / len(val_loader)
val_losses.append(avg_val_loss)
print(f"Epoch {epoch+1}: Train Loss={avg_train_loss:.4f}, Val Loss={avg_val_loss:.4f}")
# Early stopping check
#if avg_val_loss < best_val_loss:
#    best_val_loss = avg_val_loss
#    counter = 0
#    torch.save(model.state_dict(), "best_model.pth")  # save best model
#else:
#    counter += 1
#    if counter >= patience:
#        print(f"Early stopping triggered at epoch {epoch+1}")
#        break
# Load the best model after stopping
#model.load_state_dict(torch.load("best_model.pth"))
#Plot Training vs Validation loss
plt.figure(figsize=(8,5))
plt.plot(train_losses, label="Training Loss")
plt.plot(val_losses, label="Validation Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training vs Validation Loss")
plt.legend()
plt.show()
library(here)
renv::deactivate()
library(here)
source(
here::here("R/00_Config_file.R")
)
# Render -----
quarto::quarto_render(
input = here::here("Presentation/presentation.qmd")
)
fs::file_copy(
path = here::here("Presentation/index.html"),
new_path = here::here("docs/index.html"),
overwrite = TRUE
)
fs::file_copy(
path = here::here("Presentation/index.html"),
new_path = here::here("Presentation/presentation.html"),
overwrite = TRUE
)
# Clean up -----
fs::file_delete(
here::here("Presentation/index.html")
)
